{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T19:43:47.571318Z",
     "start_time": "2020-01-14T19:41:38.610757Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import random\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "%load_ext tensorboard\n",
    "!rm -rf ./logs/ \n",
    "\n",
    "# np.random.seed(785686)\n",
    "model  = None \n",
    "\n",
    "def train_augment(x: tf.Tensor, y: tf.Tensor):\n",
    "    \"\"\" apply augmentations to image x \"\"\"\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\n",
    "\n",
    "def get_model(c_out, input_shape):\n",
    "    c = 1000\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        # TODO look up how to use convolutions in tensorflow and implement them here!\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(c),\n",
    "        layers.Dense(c),\n",
    "        layers.Dense(c),\n",
    "        layers.Dense(c_out),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def view_data(dataset,labels):\n",
    "\n",
    "    print (dataset.shape)\n",
    "    print(labels.shape)\n",
    "    classes = {0,1,2,3,4,5,6,7,8,9}\n",
    "    j = 1\n",
    "    fig= plt.figure(figsize=(100,30))\n",
    "    for i in range(dataset.__len__()):\n",
    "        if classes.__contains__(labels[i][0]):\n",
    "            plt.subplot(10,1,j)\n",
    "            j+=1\n",
    "        \n",
    "        classes.remove(labels[i][0])\n",
    "        plt.imshow(dataset[i].reshape((32,32)))\n",
    "   \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    #parser = argparse.ArgumentParser(\"dnn_challenge\")\n",
    "    #parser.add_argument('--save_dir', type=str, default='./')\n",
    "    #parser.add_argument('--data_dir', type=str, default='/content/drive/My Drive/Colab Notebooks/DNN')\n",
    "    #parser.add_argument('--epochs', type=int, default=5)\n",
    "    #parser.add_argument('--batch_size', type=int, default=100)\n",
    "    #args = parser.parse_args()\n",
    "    #args.save_dir = os.path.expanduser(args.save_dir)\n",
    "    #args.data_dir = os.path.expanduser(args.data_dir)\n",
    "    \n",
    "    batch_size = 100\n",
    "    epochs = 5\n",
    "\n",
    "    global model \n",
    "\n",
    "    # load data\n",
    "    eval_data_size = 5000\n",
    "    (x_train, y_train), (x_test) = np.load(\"./WS1920_challenge_data_set.npy\", allow_pickle=True)\n",
    "\n",
    "    x_train = np.expand_dims(x_train, 4).astype('float32') / 255\n",
    "    x_eval = x_train[0:eval_data_size, ...]\n",
    "    x_train = x_train[eval_data_size:, ...]\n",
    "    y_eval = y_train[0:eval_data_size, ...]\n",
    "    y_train = y_train[eval_data_size:, ...]\n",
    "    x_test = np.expand_dims(x_test, 4).astype('float32') / 255\n",
    "    num_classes = np.max(y_train) + 1\n",
    "\n",
    "    train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(train_augment).batch(\n",
    "        batch_size).prefetch(2)\n",
    "    eval_set = tf.data.Dataset.from_tensor_slices((x_eval, y_eval)).batch(batch_size).prefetch(2)\n",
    "    test_set = tf.data.Dataset.from_tensor_slices(x_test).batch(batch_size).prefetch(2)\n",
    "\n",
    "    model = get_model(num_classes, [32, 32, 1])\n",
    "    model.summary()\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    # tensorboard writer\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "    test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "    \n",
    "    graph_logdir = 'logs/func/%s' % current_time\n",
    "    graph_writer = tf.summary.create_file_writer(graph_logdir)\n",
    "\n",
    "    @tf.function\n",
    "    def graph_trace_function(x, y):\n",
    "        with tf.GradientTape():\n",
    "            logits = model(x, training=True)\n",
    "            loss_value = loss(y, logits)\n",
    "            # when we add gradients here the graph gets quite uninterpretable\n",
    "        return loss_value\n",
    "    \n",
    "    tf.summary.trace_on(graph=True, profiler=True)\n",
    "    loss_value = graph_trace_function(tf.zeros(x_train.shape), tf.zeros(y_train.shape))\n",
    "    with graph_writer.as_default():\n",
    "        tf.summary.trace_export(\n",
    "            name=\"my_func_trace\",\n",
    "            step=0,\n",
    "            profiler_outdir=graph_logdir)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        train_loss = tf.keras.metrics.Mean()\n",
    "  \n",
    "        for i, (x, y) in enumerate(train_set):\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x, training=True)\n",
    "                loss_value = loss(y, logits)\n",
    "\n",
    "            gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "            train_accuracy.update_state(y, logits)\n",
    "            train_loss.update_state(loss_value)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', train_loss.result(), step=e)\n",
    "            tf.summary.scalar('accuracy', train_accuracy.result(), step=e)\n",
    "\n",
    "        tf.print(\"-\" * 50, output_stream=sys.stdout)\n",
    "        eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        eval_loss = tf.keras.metrics.Mean()\n",
    "        for i, (x, y) in enumerate(eval_set):\n",
    "            logits = model(x, training=False)\n",
    "            loss_value = loss(y, logits)\n",
    "            eval_accuracy.update_state(y, logits)\n",
    "            eval_loss.update_state(loss_value)\n",
    "\n",
    "        with test_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', eval_loss.result(), step=e)\n",
    "            tf.summary.scalar('accuracy', eval_accuracy.result(), step=e)\n",
    "\n",
    "        tf.print(\"epoch {0:d} \\ntrain_loss: {1:2.5f} \\ntrain_accuracy: {2:2.5f}\".format(e, train_loss.result(),\n",
    "                                                                                          train_accuracy.result()),\n",
    "                 output_stream=sys.stdout)\n",
    "        tf.print(\"eval_loss: {0:2.5f} \\neval_accuracy: {1:2.5f}\".format(eval_loss.result(),\n",
    "                                                                         eval_accuracy.result()),\n",
    "                 output_stream=sys.stdout)\n",
    "\n",
    "        train_loss.reset_states()\n",
    "        eval_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        eval_accuracy.reset_states()\n",
    "\n",
    "        # predict labels\n",
    "    if (e+1)%10 == 0:\n",
    "        predicted = []\n",
    "        for x in test_set:\n",
    "            y_ = model(x, training=False).numpy()\n",
    "            predicted.append(y_)\n",
    "        predicted = np.concatenate(predicted, axis=0)\n",
    "        predicted = np.argmax(predicted, axis=1).astype('int32')\n",
    "        predicted = np.expand_dims(predicted, 1)\n",
    "        indices = np.expand_dims(np.arange(len(predicted)), 1)\n",
    "        predicted = np.concatenate([indices, predicted], axis=1).astype('int32')\n",
    "        path = \"/content/drive/My Drive/Colab Notebooks/DNN/pred/\" +\"epochs:--\"+str(e)+str(int(time.time())) + '_predictions.csv'\n",
    "        np.savetxt(path, predicted, delimiter=\",\", header='Id,Category', fmt='%d')\n",
    "        print(\"saved predictions as: \" + path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #tf.enable_eager_execution()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-14T19:44:54.652336Z",
     "start_time": "2020-01-14T19:44:54.598534Z"
    }
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
